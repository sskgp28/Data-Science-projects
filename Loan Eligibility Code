################################## Loan Eligiblity ###################################

df = pd.read_csv('data_mobile_price_range.csv')

df.info()


dependent_variable ='loan_eligibility'
independent_varaible = list(set(df.columns.tolist())-{dependent_variable})
     

# stored the independent and dependent values in x and y variable respectively
x=df[independent_varaible].values
y=df[dependent_variable].values
     

# split the data into 4 parts x_train,x_test,y_train,y_test
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=0,stratify=y)
     
#Feature scalling


#Created an object for StandardScaler
stand = StandardScaler()
     

#Scalling
x_train = stand.fit_transform(x_train)
x_test = stand.transform(x_test)


dec =DecisionTreeClassifier(criterion='gini',
 max_depth= 10,
 max_leaf_nodes= 20,
 min_impurity_decrease= 0,
 min_samples_leaf= 1,
 min_samples_split= 2,
 splitter= 'best')
dec.fit(x_train,y_train)


#Predicting the model and checking accuracy after hyperparameter tunning
dec_pred = dec.predict(x_test)
dec_pred_accuracy = accuracy_score(dec_pred,y_test)
print(dec_pred_accuracy)
    
#Model is performing well after hyperparameter tunning.
check_overfit(dec)


#Created an object for Random forest classifier, then trained the model and checking the accuracy of the model.
rf_tune = RandomForestClassifier(max_depth= 15,
 max_features= 20,
 min_samples_leaf= 1,
 min_samples_split= 2,
 n_estimators= 200)
rf_tune.fit(x_train,y_train)
rf_tune_pred = rf_tune.predict(x_test)
rf_tune_accuracy = accuracy_score(rf_tune_pred,y_test)
rf_tune_accuracy




#find the value of k.
error=[]
for i in range(1,1000):
  knn = KNeighborsClassifier(n_neighbors=i)
  knn.fit(x_train,y_train)
  knn_pred = knn.predict(x_test)
  error.append(np.mean(knn_pred!=y_test))
     

#Plotting line plot.
plt.figure(figsize=(10,6))
plt.plot(range(1,1000),error,color='black',linestyle='dashed',marker='o',markerfacecolor='red')
plt.title('Error rate vs  K')
plt.xlabel('k value')
plt.ylabel('Error')
plt.show()
print("minimum error ",min(error),"at the value of k =",error.index(min(error))+1)
     


#Created an object for KNN.
knn = KNeighborsClassifier(n_neighbors=273,p=1,weights='distance',metric= 'manhattan')
     

# Trained the model
knn.fit(x_train,y_train)
     
KNeighborsClassifier(metric='manhattan', n_neighbors=273, p=1,
                     weights='distance')
#In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
#On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.

#Model Prediction
knn_pred = knn.predict(x_test)
     

#Checking accuracy
knn_accuracy = accuracy_score(knn_pred,y_test)
knn_accuracy


#Learning Curve
check_overfit(knn)


#Set the parameters in the model
parameter = {'penalty':['l1', 'l2', 'elasticnet'],'solver':['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],'C':[0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10]}
     

#hyperparameter tuning in the model 
logi = LogisticRegression()
logi = GridSearchCV(logi,param_grid=parameter,scoring='accuracy',cv=5)
     

#Trained the model
logi.fit(x_train,y_train)




#Got the best parameter
logi.best_params_
     
{'C': 1, 'penalty': 'l1', 'solver': 'saga'}

#Applied best parameter
logi_tune = LogisticRegression(C= 1, penalty= 'l1', solver= 'saga')
     

#Trained the using best parameters 
logi_tune.fit(x_train,y_train)



#Predicting the model and checked the accuracy of the model
logi_tune_pred = logi_tune.predict(x_test)
logi_tune_accuracy = accuracy_score(logi_tune_pred,y_test)
logi_tune_accuracy


#Learning Curve 
check_overfit(logi_tune)



nb=GaussianNB()
     

#Trained the model 
nb.fit(x_train,y_train)
     
GaussianNB()

#Model Prediction 
nb_pred = nb.predict(x_test)
     

#Checked the accuracy of the model
nb_accuracy = accuracy_score(nb_pred,y_test)
nb_accuracy



#Set the parameters
params={
 "learning_rate"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,
 "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],
  "min_child_weight" : [ 1, 3, 5, 7 ],
  "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
 "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ]
    
 }
     

#Hyperparameter tunning and trained the model 
xgb= XGBClassifier()


random_search= RandomizedSearchCV(xgb,param_distributions=params,n_iter=5,scoring='accuracy',n_jobs=-1,cv=5,verbose=3)
random_search.fit(x_train,y_train)


#Model prediction 
xgb_pred = random_search.predict(x_test)
     

#Checked the accuracy
xg_boost_accuracy = accuracy_score(xgb_pred,y_test)
     

xg_boost_accuracy


#Set base models and final estimator
estimators = [('gbc',GradientBoostingClassifier()),('xgb',XGBClassifier(colsample_bytree=0.7, gamma=0.3, learning_rate=0.25, max_depth=15,
              min_child_weight=3, objective='multi:softprob')),
       ('bc',BaggingClassifier(
 bootstrap= True,
 bootstrap_features= True,
 max_features= 1.0,
 max_samples= 0.8,
 n_estimators= 20))]
     

#Hyper parameter tunning
sc= StackingClassifier(estimators=estimators,passthrough=True,n_jobs=-1)
     

#Trained the model 
sc.fit(x_train,y_train)


#Model prediction 
sc_pred = sc.predict(x_test)
     

#Checked accuracy
sc_accuracy=accuracy_score(sc_pred,y_test)
     

sc_accuracy
     
#Learning Curve
check_overfit(sc)

#set all the accuracy in data frame
evaluation = pd.DataFrame(data={'Model':['KNN classifier','DecisionTree classifier','LogisticRegression classifier','RandomForest classifier','Naive Bays','xgboost'], 'accuracy':[knn_accuracy,dec_pred_accuracy,logi_tune_accuracy,rf_tune_accuracy,nb_accuracy,xg_boost_accuracy]})

     

#sorting
evaluation=evaluation.sort_values(by=['accuracy'],ascending=False).reset_index(drop=True)

evaluation
